{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "obbbvn35fc7owxmeik3m",
   "authorId": "233194668061",
   "authorName": "PLAKHANPAL",
   "authorEmail": "Puneet.Lakhanpal@Snowflake.com",
   "sessionId": "5e774511-e974-4eb8-a0f9-383782520a5d",
   "lastEditTime": 1741240132087
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d9736dc-2c27-4e68-bdae-ebb724531507",
   "metadata": {
    "name": "intro",
    "collapsed": false
   },
   "source": "# Distributed Multi-Node, Multi-GPU Audio Transcription in ML Container Runtime"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "import_libs"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\nimport torch\n# We can also use Snowpark for our analyses!\nfrom typing import Dict\nfrom pathlib import Path\nimport numpy as np\nimport shutil\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.ml.ray.datasource import SFStageBinaryFileDataSource\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom snowflake.ml.runtime_cluster import scale_cluster, get_nodes\nfrom snowflake.ml.ray.datasink import SnowflakeTableDatasink\nimport ray\nimport subprocess\nimport logging\nsession = get_active_session()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a55da4f5-449a-4c69-9ef0-3a5f70421e17",
   "metadata": {
    "name": "intro_see_starter_nodes",
    "collapsed": false
   },
   "source": "### Start with one node"
  },
  {
   "cell_type": "code",
   "id": "b23ce5cd-c2b0-41fa-bcec-4a77fda4cf0e",
   "metadata": {
    "language": "python",
    "name": "initialize_ray_and_start"
   },
   "outputs": [],
   "source": "ray.init(ignore_reinit_error=True)\nnum_nodes = len([node for node in ray.nodes() if node[\"Alive\"]==True])\nprint(num_nodes)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "df11d409-2cb0-4107-902b-502699d54fe5",
   "metadata": {
    "name": "intro_scale_to_5_nodes",
    "collapsed": false
   },
   "source": "### Scale up to 5 nodes"
  },
  {
   "cell_type": "code",
   "id": "efebb414-e8f2-46ee-adda-173da0ced783",
   "metadata": {
    "language": "python",
    "name": "scale_to_5_nodes",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "num_nodes = 5\nscale_cluster('\"Audio Processing - Distributed Inferencing\"', num_nodes)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c66019ea-840e-490b-bc91-30112590595f",
   "metadata": {
    "name": "intro_control_ray_logging",
    "collapsed": false
   },
   "source": "### Control ray logging"
  },
  {
   "cell_type": "code",
   "id": "fa5778d7-1e10-4831-9757-1410fdc7a383",
   "metadata": {
    "language": "python",
    "name": "control_ray_logging"
   },
   "outputs": [],
   "source": "def configure_ray_logger() -> None:\n    #Configure Ray logging\n    ray_logger = logging.getLogger(\"ray\")\n    ray_logger.setLevel(logging.CRITICAL)\n\n    data_logger = logging.getLogger(\"ray.data\")\n    data_logger.setLevel(logging.CRITICAL)\n\n    #Configure root logger\n    logger = logging.getLogger()\n    logger.setLevel(logging.CRITICAL)\n\n    #Configure Ray's data context\n    context = ray.data.DataContext.get_current()\n    context.execution_options.verbose_progress = False\n    context.enable_operator_progress_bars = False\n\nconfigure_ray_logger()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7bee4cc4-a6eb-443a-a34c-66fddbfcc151",
   "metadata": {
    "language": "python",
    "name": "install_ffmpeg_each_node",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "@ray.remote(num_cpus=0)  # Ensures task does not consume CPU slots\ndef install_ffmpeg():\n    try:\n        # Prevents interactive prompts\n        env = {\"DEBIAN_FRONTEND\": \"noninteractive\"}\n        \n        # Install ffmpeg silently\n        subprocess.run([\"apt-get\", \"update\"], check=True, env=env)\n        subprocess.run(\n            [\"apt-get\", \"install\", \"-y\", \"ffmpeg\"],\n            check=True,\n            stdin=subprocess.DEVNULL,  # Prevents any user input\n            stdout=subprocess.PIPE,  # Hides output\n            stderr=subprocess.PIPE,  # Hides errors unless needed\n            env=env\n        )\n\n        # Verify installation\n        result = subprocess.run([\"ffmpeg\", \"-version\"], capture_output=True, text=True, check=True)\n        return f\"✅ ffmpeg installed on {ray.util.get_node_ip_address()}:\\n{result.stdout.splitlines()[0]}\"\n    except subprocess.CalledProcessError as e:\n        return f\"❌ Failed on {ray.util.get_node_ip_address()}: {e}\"\n\n# Get unique node IPs in the cluster\nnodes = {node[\"NodeManagerAddress\"] for node in ray.nodes() if node[\"Alive\"]}\n\n# Install ffmpeg on each unique node\ntasks = [install_ffmpeg.options(resources={f\"node:{node}\": 0.01}).remote() for node in nodes]\nresults = ray.get(tasks)\n\n# Print results\nfor res in results:\n    print(res)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9abca883-cf62-4603-ad55-3273b4c0806b",
   "metadata": {
    "language": "python",
    "name": "print_gpus_in_ray_cluster",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "print(int(ray.cluster_resources()['GPU']))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "416d5937-05b5-473a-b3b3-b58d3e5c998a",
   "metadata": {
    "name": "intro_see_audio_files",
    "collapsed": false
   },
   "source": "### See audio files in snowflake stage"
  },
  {
   "cell_type": "code",
   "id": "938569bd-67ca-4657-b922-b126aef46f91",
   "metadata": {
    "language": "sql",
    "name": "see_audio_files",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "ls @AUDIO_FILES_STAGE",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0f5905b6-5665-4c25-a44f-747b3b86fd7f",
   "metadata": {
    "language": "python",
    "name": "get_ray_dataset_using_snow_apis",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "audio_source = SFStageBinaryFileDataSource(\n    stage_location = \"@AUDIO_FILES_STAGE/\",\n    database = session.get_current_database(),\n    schema = session.get_current_schema(),\n    file_pattern = \"*.flac\"\n)\n\n# Load audio files into a ray dataset\naudio_dataset = ray.data.read_datasource(audio_source)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d7da5a52-bcd4-4109-8a33-00bd20c19504",
   "metadata": {
    "language": "python",
    "name": "see_sample_audio_files",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "audio_dataset.show(1)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4ae5462d-e0e4-4396-b383-1c7e1f7645b9",
   "metadata": {
    "language": "python",
    "name": "see_count_of_audio_files",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "audio_dataset.count()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "12dcd965-ea6d-4850-9786-44c274495bd3",
   "metadata": {
    "name": "intro_get_whisper_model",
    "collapsed": false
   },
   "source": "### Get whisper model"
  },
  {
   "cell_type": "code",
   "id": "b1605a95-9fc0-4511-ae2b-7343208a242a",
   "metadata": {
    "language": "python",
    "name": "set_model_params",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "model_id = \"openai/whisper-large-v3\"\nbatch_size = 30\nis_cuda_available = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if is_cuda_available else \"cpu\")\ntorch_dtype = torch.float16 if is_cuda_available else torch.float32\nprint(device)\nprint(torch_dtype)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c19ddbbe-3cdc-4964-91ef-8c316dab899e",
   "metadata": {
    "name": "intro_distributed_inferencing",
    "collapsed": false
   },
   "source": "### Distributed inferencing"
  },
  {
   "cell_type": "code",
   "id": "027b9201-d3e2-492b-b525-b78335d57a4f",
   "metadata": {
    "language": "python",
    "name": "audio_transcription_class"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport tempfile\nimport os\n\nclass TranscribeAudioUpdated:\n    def __init__(self):\n        # initialize model here so that model can be put into correct GPU/node\n        model = AutoModelForSpeechSeq2Seq.from_pretrained(\n            model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n        )\n        model.to(device)\n        processor = AutoProcessor.from_pretrained(model_id)\n        self.pipe = pipeline(\n            \"automatic-speech-recognition\",\n            model=model,\n            tokenizer=processor.tokenizer,\n            feature_extractor=processor.feature_extractor,\n            max_new_tokens=128,\n            chunk_length_s=30,\n            batch_size=batch_size,\n            return_timestamps=True,\n            torch_dtype=torch_dtype,\n            device=device,\n            generate_kwargs={\"language\": \"english\"}\n        )\n\n    def __call__(self, batch: pd.DataFrame) -> pd.DataFrame:\n        temp_files = []\n        try:\n            # Write each binary to a temporary file.\n            for binary_content in batch[\"file_binary\"]:\n                # Use an appropriate suffix (e.g., .wav or .flac) based on your audio format.\n                tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".flac\")\n                tmp_file.write(binary_content)\n                tmp_file.close()\n                temp_files.append(tmp_file.name)\n            \n            # Use the temporary file paths for inference.\n            predictions = self.pipe(temp_files)\n            assert len(predictions) == len(batch)\n            outputs = [str(generated_audio[\"text\"]).strip() for generated_audio in predictions]\n            batch['outputs'] = outputs\n            batch.drop(columns=['file_binary'], inplace=True)\n        finally:\n            # Clean up temporary files.\n            for file_path in temp_files:\n                try:\n                    os.remove(file_path)\n                except OSError:\n                    pass\n        return batch",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c2b1dec8-2eec-4874-93b2-7771b973f0f4",
   "metadata": {
    "language": "python",
    "name": "map_batches_of_audio_files",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "transcribed_ds = audio_dataset.map_batches(TranscribeAudioUpdated,\n        batch_size=batch_size,\n        batch_format='pandas',\n        concurrency=5,\n        num_gpus=1,\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dff09d59-99ab-4ca7-9d27-bb17133dcb59",
   "metadata": {
    "language": "sql",
    "name": "drop_results_table_if_exists"
   },
   "outputs": [],
   "source": "drop table if exists WHISPER_DEMO_OUTPUT",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "99b998b9-ea51-4536-9da2-9539f3615f4d",
   "metadata": {
    "language": "python",
    "name": "output_to_a_snowflake_table"
   },
   "outputs": [],
   "source": "datasink = SnowflakeTableDatasink(\n    table_name=\"WHISPER_DEMO_OUTPUT\",\n    database=session.get_current_database(),\n    schema=session.get_current_schema(),\n    auto_create_table=True\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e10e0495-e8aa-483e-8bff-d7797e099afa",
   "metadata": {
    "language": "python",
    "name": "write_transcriptions_to_snowflake",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "transcribed_ds.write_datasink(datasink)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "73cf7b38-3fb7-4dad-ad5f-2415f6eb5788",
   "metadata": {
    "language": "python",
    "name": "see_results",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "session.table(\"WHISPER_DEMO_OUTPUT\").show()",
   "execution_count": null
  }
 ]
}